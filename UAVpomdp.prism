// POMDP model
// UAV collision avoidance
// discrete states
// test
// feng weizhi

// state space (value of variable "(x, y)")
// s = (x1, y1, x2, y2)
// (x1, y1) is the position of own aircraft
// (x2, y2) is the position of intruder
//  -10 <= x <= 10
// 0 <= y <= 30
// (0, 30) is the target

pomdp

const int detectRange = 10 * 10;
const int NMAC = 3 * 3;

// can observe the direction of intruder relative to own
observables
	o : [0.. 2]
endobservables

// assume the heading of own aircraft is always in the positive y-axis direction
// o = 1: Left, (x2 - x1)^2 + (y2 - y1)^2 <= detectRange * detectRange & x2 <= x1
// o = 2: Right, (x2 - x1)^2 + (y2 - y1)^2 <= detectRange * detectRange & x2 > x1
// o = 0: No-detection, (x2 - x1)^2 + (y2 - y1)^2 > detectRange * detectRange

// intruder
// randomly choose an action from "go ahead", "turn left" and "turn right"
// Markov chain
module intruder

	x2 : [-10..10] init 0;
	y2 : [0.. 30] init 15; 

 	-10 <= x2 <= 10 & y2 = 0 -> (x2' = x2) & (y2' = y2)

	x2 = -10 & y2 > 0 -> 1/2 : (x2' = x2 & y2' = y2 - 1) + 1/2 : (x2' = x2 + 1 & y2');

	x2 = 10 & y2 > 0 -> 1/2 : (x2' = x2 & y2' = y2 - 1) + 1/2 : (x2' = x2 - 1 & y2' = y2);

	-10 < x2 < 10 & y2 > 0 -> 1/3 : (x2' = x2 & y2' = y2 -1) + 1/3 : (x2' = x2 + 1 & y2' = y2) + 1/3 : (x2' = x2 - 1 & y2' = y2)
endmodule

// own aircraft
// [action] curr state -> next state distribution & observable distribution
// [action] guard -> prob_state_1 : ( next_state_1 & (prob_observ_1 : next_observ_1 + ... + prob_observ_n : next_observ_n))
//  + ... + prob_state_m : (next_state_m & (prob_observ_1 : next_observ_1 + ... + prob_observ_n : next_observ_n));
module own

	x1 : [-10.. 10] init 0;
	y1 : [0.. 30] init 0;
	o : [0.. 2] init 0;

	formula dis = (x2 - x1)^2 + (y2 - y1)^2;

	// moving: turn left, turn right and go ahead
	// state transition probablity: Pr((x1' = x1 -1, y1' = y1) | (x1, y1), turnLeft) = 1, others are the same;
	// observable probablity: Pr(left | x2 <= x1 & dis <= detectRange) = 0.8;
	// observation matrix:
	// --------------- Right ------ Left ------ No-detection ------------
	//  x2 <= x1        0.8                0.1                    0.1
	//  x2 > x1          0.1                0.8                    0.1
	// dis > 100       0.1                0.1                    0.8

	// turn left and observe left, right, no-detection
	[turnLeft] -10 < x1 <= 10 & dis <= detectRange & x2 <= x1 -> (x1' = x1 - 1) & (y1' = y1) & (0.8 : (o' = 1) + 0.1 : (o' = 2) + 0.1 : (o' = 0));
	[turnLeft] -10 < x1 <= 10 & dis <= detectRange & x2 > x1 -> (x1' = x1 - 1) & (y1' = y1) & (0.1 : (o' = 1) + 0.8 : (o' = 2) + 0.1 : (o' = 0));
	[turnLeft] -10 < x1 <= 10 & dis > detectRange -> (x1' = x1 - 1) & (y1' = y1) & (0.1 : (o' = 1) + 0.1 : (o' = 2) + 0.8 : (o' = 0));

	// turn right
	[turnRight] -10 <= x1 < 10 & dis <= detectRange & x2 <= x1 -> (x1' = x1 + 1) & (y1' = y1) & (0.8 : (o' = 1) + 0.1 : (o' = 2) + 0.1 : (o' = 0));
	[turnRight] -10 <= x1 < 10 & dis <= detectRange & x2 > x1 -> (x1' = x1 + 1) & (y1' = y1) & (0.1 : (o' = 1) + 0.8 : (o' = 2) + 0.1 : (o' = 0));
	[turnRight] -10 <= x1 < 10 & dis > detectRange -> (x1' = x1 + 1) & (y1' = y1) & (0.1 : (o' = 1) + 0.1 : (o' = 2) + 0.8 : (o' = 0));

	// goAhead
	[goAhead] -10 <= x1 < 10 & dis <= detectRange & x2 <= x1 -> (x1' = x1) & (y1' = y1 + 1) & (0.8 : (o' = 1) + 0.1 : (o' = 2) + 0.1 : (o' = 0));
	[goAhead] -10 <= x1 < 10 & dis <= detectRange & x2 > x1 -> (x1' = x1) & (y1' = y1 + 1) & (0.1 : (o' = 1) + 0.8 : (o' = 2) + 0.1 : (o' = 0));
	[goAhead] -10 <= x1 < 10 & dis > detectRange -> (x1' = x1) & (y1' = y1 + 1) & (0.1 : (o' = 1) + 0.1 : (o' = 2) + 0.8 : (o' = 0));
																												

	// reach the target
	[done] x1 = 0 & y1 = 30 -> true;

endmodule

	
endmodule

// reward 
// [action] guard : reward;
rewards

	(x1 - x2)^2 + (y1 - y2)^2 <= NMAC * NMAC : -10000;
	true : 100; 
	[turnLeft]  !(x1 = 0 & y1 = 30): -1;
	[turnRight]  !(x1 = 0 & y1 = 30): -1;
	[goAhead] !(x1 = 0 & y1 = 30): 0;  

endrewards



